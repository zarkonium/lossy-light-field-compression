<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>Lossy Light Field Compression Using Modern Deep Learning and Domain Randomization Techniques</title>
	<meta property="og:image" content="gradcam.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Lossy Light Field Compression Using Modern Deep Learning and Domain Randomization Techniques." />
	<meta property="og:description" content="Paper description." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">Lossy Light Field Compression Using Modern Deep Learning and Domain Randomization Techniques</span>
		<table align=center width=600px>
			<table align=center width=600px>
				<tr>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://zarkonium.github.io/">Svetozar Zarko Valtchev</a></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://scholar.google.ca/citations?user=Ox-xAuIAAAAJ&hl=en">Jianhong Wu</a></span>
						</center>
					</td>
<!-- 					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://en.wikipedia.org/wiki/James_J._Gibson">Third Author</a></span>
						</center>
					</td> -->
				</tr>
			</table>
			<span style="font-size:24px"><b>York University, Toronto</b></span>
			<table align=center width=250px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='http://zarkonium.github.io/files/Domain_Randomization_for_Neural_Network_Classification_Published.pdf'>[Paper]</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://www.kaggle.com/datasets/zarkonium/synthetic-image-dataset-cats-dogs-bikes-cars'>[Dataset]</a></span>
						</center>
					</td>
<!-- 					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://github.com/richzhang/webpage-template'>[GitHub]</a></span><br>
						</center>
					</td> -->
				</tr>
			</table>
		</table>
	</center>

	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:1200px; margin-left: -80px;" src="./resources/gradcam.png"/>
					</center>
				</td>
			</tr>
		</table>
		<table align=center width=1000px>
			<tr>
				<td>
					<span style="font-size:14px">
						<i>First 4 real and synthetic images in each category, and their subsequent analysis as to what each model is looking at to classify each image (real model on the left, synthetic on the right). Second row shows the GradCAM explainer while the third row utilizes Occulusion Sensitivity (OS), when testing for the cat class.</i>
					</span>
				</td>
			</tr>
		</table>
	</center>
	
	<br>
	<br>
	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
<!-- 				Large data requirements are often the main hurdle in training neural networks. Synthetic data is a cheap and efficient solution to assemble such large datasets. Using domain randomization, we show that a sufficiently well generated synthetic image dataset can be used to train a neural network classifier, achieving accuracy levels as high as 88% on 2 category classification. We show that the most important domain randomization parameter is a large variety of subjects, while secondary parameters such as lighting and textures are not. Based on our results, there is reason to believe that models trained on domain randomized images transfer to new domains better than those trained on real photos. Model performance seems to diminish slightly as the number of categories increases. -->
			</td>
		</tr>
	</table>
	
	Large data requirements are often the main hurdle in training neural networks. Synthetic data is a cheap and efficient solution to assemble such large datasets. Using domain randomization, we show that a sufficiently well generated synthetic image dataset can be used to train a neural network classifier, achieving accuracy levels as high as 88% on 2 category classification. We show that the most important domain randomization parameter is a large variety of subjects, while secondary parameters such as lighting and textures are not. Based on our results, there is reason to believe that models trained on domain randomized images transfer to new domains better than those trained on real photos. Model performance seems to diminish slightly as the number of categories increases.
	<br>
	<br>
	<hr>
	<center><h1>Talk</h1></center>
	<p align="center">
		<iframe width="660" height="395" src="https://www.youtube.com/embed/dQw4w9WgXcQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen align="center"></iframe>
	</p>

	<table align=center width=800px>
		<br>
		<tr>
			<center>
				<span style="font-size:14pt"><a href=''>[Slides]</a>
				</span>
			</center>
		</tr>
	</table>
	
	<br>
	<hr>
	
	<center><h1>Dataset</h1></center>
	
	We generate 25,000 synthetic images with labels of cats, dogs, cars and bicycles. The dataset is publically avaiable on <a href="https://www.kaggle.com/datasets/zarkonium/synthetic-image-dataset-cats-dogs-bikes-cars">Kaggle</a>. Please cite the publication if you choose to use the dataset.
	
	<table align=center width=1000px>
		<center>
			<tr>
				<td align=center width=400px>
					<img class="round" style="width:850px" src="./resources/dataExample2.png"/>
				</td>
			</tr>
		</center>
	</table>
	
	<table align=center width=800px>
		<br>
		<tr>
			<center>
				<span style="font-size:14pt"><a href='https://www.kaggle.com/datasets/zarkonium/synthetic-image-dataset-cats-dogs-bikes-cars'>[Dataset]</a>
				</span>
			</center>
		</tr>
	</table>
	
	<br>
	<hr>

	<center><h1>Results</h1></center>
	
	We found that the most important parameter in the accuracy of the neural-based classifier was the variety of breeds of the subjects. All other parameters had marginal effects on the overall accuracy, with the occluders and the variation of poses actually having negative consequences. This is in line with what we expect, obstructing parts of the subject should cause important features to be hidden, as is the case with the obstructors and some specific poses. Results remained consistent as we increased the number of categories in the classification task, adding car and bike images to the experiments.

	<table align=center width=1000px>
		<center>
			<tr>
				<td align=center width=400px>
					<img class="round" style="width:530px; margin-top: 92px;" src="./resources/parameters.png"/>
				</td>
				<td align=center width=400px>
					<img class="round" style="width:600px" src="./resources/numberCategories2.png"/>
				</td>
			</tr>
		</center>
	</table>
<!-- 	<table align=center width=400px>
		<tr>
			<td align=center width=400px>
				<center>
					<td><img class="round" style="width:450px" src="./resources/gradcam.png"/></td>
				</center>
			</td>
		</tr>
	</table> -->
<!-- 	<table align=center width=850px>
		<center>
			<tr>
				<td>
					Short description if wanted
				</td>
			</tr>
		</center>
	</table> -->
<!-- 	<table align=center width=800px>
		<br>
		<tr><center>
			<span style="font-size:28px">&nbsp;<a href='https://github.com/richzhang/webpage-template'>[GitHub]</a>
			</center>
		</span>
	</table> -->
	<br>
	<hr>
	<table align=center width=540px>
		<center><h1>Paper and Supplementary Material</h1></center>
		<tr>
			<td><a href="http://zarkonium.github.io/files/Domain_Randomization_for_Neural_Network_Classification_Published.pdf"><img class="layered-paper-big" style="height:175px" src="./resources/paperPreview.png"/></a></td>
			<td><span style="font-size:14pt">S.Z. Valtchev, J. Wu.<br>
				<b>Lossy Light Field Compression Using Modern Deep Learning and Domain Randomization Techniques.</b><br>
				SpringerOpen Journal of Big Data, 2021.<br>
<!-- 				(hosted on <a href="">ArXiv</a>)<br> -->
				<!-- (<a href="./resources/camera-ready.pdf">camera ready</a>)<br> -->
				<span style="font-size:4pt"><a href=""><br></a>
				</span>
			</td>
		</tr>
	</table>
	<br>

	<table align=center width=600px>
		<tr>
			<td><span style="font-size:14pt"><center>
				<a href="./resources/bibtex.txt">[Bibtex]</a>
			</center></td>
		</tr>
	</table>
	
	<br>
	<hr>
	<br>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					This work has been supported by the Natural Sciences and Engineering Research Council of Canada, and by the Canada Research Chairs program.
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>

