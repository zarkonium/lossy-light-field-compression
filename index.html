<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1170px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>Lossy Light Field Compression Using Modern Deep Learning and Domain Randomization Techniques</title>
	<meta property="og:image" content="gradcam.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Lossy Light Field Compression Using Modern Deep Learning and Domain Randomization Techniques." />
	<meta property="og:description" content="Paper description." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">Lossy Light Field Compression Using Modern Deep Learning and Domain Randomization Techniques</span>
		<table align=center width=600px>
			<table align=center width=600px>
				<tr>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://zarkonium.github.io/">Svetozar Zarko Valtchev</a></span>
						</center>
					</td>
<!-- 					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://scholar.google.ca/citations?user=Ox-xAuIAAAAJ&hl=en">Jianhong Wu</a></span>
						</center>
					</td> -->
<!-- 					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://en.wikipedia.org/wiki/James_J._Gibson">Third Author</a></span>
						</center>
					</td> -->
				</tr>
			</table>
			<span style="font-size:24px"><b>York University, Toronto</b></span>
			<table align=center width=400px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href=''>[Paper]</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href=''>[Dataset]</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://docs.google.com/presentation/d/1_0NCR8sSxQbIKBkJnCIyxIxlYcyZhomloV_NeUQedxg/edit?usp=sharing'>[Slides]</a></span>
						</center>
					</td>
<!-- 					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://github.com/richzhang/webpage-template'>[GitHub]</a></span><br>
						</center>
					</td> -->
				</tr>
			</table>
		</table>
	</center>
	
	<br>

	<center>
		<table align=center width=1000px>
			<center>
				<tr>
					<td align=center width=400px>
						<img class="round" style="width: 380; height: 380; object-fit: none;" src="./resources/origami_full_ours_4.5.gif"/>
						Ours
					</td>
					<td align=center width=400px>
						<img class="round" style="width: 380; height: 380; object-fit: none;" src="./resources/origami_full_oursCNE_4.5.gif"/>
						Ours + CNE
					</td>
					<td align=center width=400px>
						<img class="round" style="width: 380; height: 380; object-fit: none;" src="./resources/origami_full.gif"/>
						True
					</td>
				</tr>
				<tr>
					<td align=center width=400px>
						<img class="round" style="width: 380; height: 380; object-fit: none;" src="./resources/origami_zoomed_ours_4.5.gif"/>
						Ours
					</td>
					<td align=center width=400px>
						<img class="round" style="width: 380; height: 380; object-fit: none;" src="./resources/origami_zoomed_oursCNE_4.5.gif"/>
						Ours + CNE
					</td>
					<td align=center width=400px>
						<img class="round" style="width: 380; height: 380; object-fit: none;" src="./resources/origami_zoomed.gif"/>
						True
					</td>
				</tr>
				<tr>
					<td align=center width=400px>
						<img class="round" style="width: 380; height: 380; object-fit: none;" src="./resources/bedroom_full_oursCNE_3.0.gif"/>
					</td>
					<td align=center width=400px>
						<img class="round" style="width: 380; height: 380; object-fit: none;" src="./resources/herbs_full_oursCNE_4.5.gif"/>
					</td>
					<td align=center width=400px>
						<img class="round" style="width: 380; height: 380; object-fit: none;" src="./resources/bicycle_full_oursCNE_4.5.gif"/>
					</td>
				</tr>
			</center>
		</table>
		<table align=center width=1000px>
			<tr>
				<td>
					<span style="font-size:14px">
						<i>Examples of the full and zoomed-in Origami scene from the <a href='https://lightfield-analysis.uni-konstanz.de/'>HCI Light Field Dataset</a> reconstructed using our method, with and without the addition of the Convolutional Neural Enhancer (CNE), at a bitrate of 0.2bpp.</i>
					</span>
				</td>
			</tr>
		</table>
	</center>
	
	<br>
	<br>
	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
<!-- 				Large data requirements are often the main hurdle in training neural networks. Synthetic data is a cheap and efficient solution to assemble such large datasets. Using domain randomization, we show that a sufficiently well generated synthetic image dataset can be used to train a neural network classifier, achieving accuracy levels as high as 88% on 2 category classification. We show that the most important domain randomization parameter is a large variety of subjects, while secondary parameters such as lighting and textures are not. Based on our results, there is reason to believe that models trained on domain randomized images transfer to new domains better than those trained on real photos. Model performance seems to diminish slightly as the number of categories increases. -->
			</td>
		</tr>
	</table>
	
	Lossy data compression is a particular type of informational encoding utilizing approximations in order to efficiently tradeoff accuracy in favour of smaller file sizes. The transmission and storage of images is a typical example of this in the modern digital world. However the reconstructed images often suffer from degradation and display observable visual artifacts. Convolutional Neural Networks (CNNs) have garnered much attention in all corners of Computer Vision (CV), including the tasks of image compression and artifact reduction. We study how lossy compression can be extended to higher dimensional images with varying viewpoints, known as light fields. Domain Randomization (DR) is explored in detail, and used to generate the largest light field dataset we are aware of, to be used as training data. We formulate the task of compression under the frameworks of neural networks and calculate a quantization tensor for the 4-D Discrete Cosine Transform (DCT) coefficients of the light fields. In order to accurately train the network, a high degree approximation to the rounding operation is introduced. In addition, we present a multi-resolution convolutional-based light field enhancer, producing average gains of 0.854 db in Peak Signal-to-Noise Ratio (PSNR), and 0.0338 in Structual Similarity Index Measure (SSIM) over the base model, across a wide range of bitrates.
	<br>
	<br>
	<hr>
	<center><h1>Talk</h1></center>
	<p align="center">
<!-- 		<iframe width="660" height="395" src="https://www.youtube.com/embed/dQw4w9WgXcQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen align="center"></iframe> -->
		<iframe src="https://docs.google.com/presentation/d/e/2PACX-1vSbS-Ol8DSOSPfwOYPiA7eawzdAZgLU_-dFf-yZ0S-ez72xzp8o-4bNqXFabIouTxhoSPc-8AHWa-j8/embed?start=false&loop=false&delayms=3000" frameborder="0" width="960" height="569" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>
	</p>

	<table align=center width=800px>
		<br>
		<tr>
			<center>
				<span style="font-size:14pt"><a href='https://docs.google.com/presentation/d/1_0NCR8sSxQbIKBkJnCIyxIxlYcyZhomloV_NeUQedxg/edit?usp=sharing'>[Slides]</a>
				</span>
			</center>
		</tr>
	</table>
	
	<br>
	<hr>
	
	<center><h1>LIAM LF Dataset</h1></center>
	
	We produce 20,000 synthetic light fields, at a 512x512 spatial resolution and a 9x9 angular resolution, as can be seen in the examples below. The dataset also includes depth maps and segmentation maps for the central Subaperture Image of each light field, as well as camera instrinsics data. A small part of the dataset is publically avaiable on <a href="">Kaggle</a>, while the full dataset is available on request. Please cite the publication if you use the LIAM-LF-Dataset in your research.
	<br>
	<br>
	
	<table align=center width=1000px>
		<center>
			<tr>
				<td align=center width=400px>
					<img class="round" style="width:360px" src="./resources/example1.png"/>
				</td>
				<td align=center width=400px>
					<img class="round" style="width:360px" src="./resources/example2.png"/>
				</td>
				<td align=center width=400px>
					<img class="round" style="width:360px" src="./resources/example3.png"/>
				</td>
			</tr>
			<tr>
				<td align=center width=400px>
					<img class="round" style="width:360px" src="./resources/example1st.png"/>
				</td>
				<td align=center width=400px>
					<img class="round" style="width:360px" src="./resources/example2st.png"/>
				</td>
				<td align=center width=400px>
					<img class="round" style="width:360px" src="./resources/example3st.png"/>
				</td>
			</tr>
		</center>
	</table>
	
	<center>
		<img class="round" style="width:860px" src="./resources/liam_lf_dataset.png"/>
	</center>
	
	<table align=center width=800px>
		<br>
		<tr>
			<center>
				<span style="font-size:14pt"><a href=''>[Dataset]</a>
				</span>
			</center>
		</tr>
	</table>
	
	<br>
	<hr>

	<table align=center width=540px>
		<center><h1>Paper and Supplementary Material</h1></center>
		<tr>
			<td><a href=""><img class="layered-paper-big" style="height:175px" src="./resources/paperPreview2.png"/></a></td>
			<td><span style="font-size:14pt">S.Z. Valtchev.<br>
				<b>Lossy Light Field Compression Using Modern Deep Learning and Domain Randomization Techniques.</b><br>
				YorkSpace, 2022.<br>
<!-- 				(hosted on <a href="">ArXiv</a>)<br> -->
				<!-- (<a href="./resources/camera-ready.pdf">camera ready</a>)<br> -->
				<span style="font-size:4pt"><a href=""><br></a>
				</span>
			</td>
		</tr>
	</table>
	<br>

	<table align=center width=600px>
		<tr>
			<td><span style="font-size:14pt"><center>
				<a href="./resources/bibtex.txt">[Bibtex]</a>
			</center></td>
		</tr>
	</table>
	
	<br>
	<hr>
	<br>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					This work has been supported by the Natural Sciences and Engineering Research Council of Canada, and by the Canada Research Chairs program.
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>

